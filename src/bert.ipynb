{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bc420",
   "metadata": {},
   "source": [
    "Generate a list of topics that will be used as reference for labeling and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cadcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Define initial topic list\n",
    "initial_topics = [\n",
    "    \"Machine Learning\", \"Deep Learning\", \"Artificial Intelligence\", \"Cybersecurity\",\n",
    "    \"Mobile App Development\", \"Web Development\", \"IoT, Internet of Things\",\n",
    "    \"Cloud Computing\", \"Data Science\", \"Big Data\", \"Blockchain\", \"Augmented Reality\",\n",
    "    \"Virtual Reality\", \"Natural Language Processing\", \"Computer Vision\", \"Robotics\",\n",
    "    \"Networking\", \"Database Management\", \"Computer Security\",\n",
    "]\n",
    "\n",
    "def clean_text(s, comma=False):\n",
    "    s = s.lower()\n",
    "    s = s.split()\n",
    "    s = \" \".join(s)\n",
    "    # Add a comma if there is any '\\' or '/' in the string\n",
    "    if comma:\n",
    "        s = re.sub(r'\\\\', ',', s)\n",
    "        s = re.sub(r'/', ',', s)\n",
    "    else:\n",
    "        s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n",
    "    return s\n",
    "\n",
    "def remove_stop_words(s):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    s = s.split()\n",
    "    s = [w for w in s if not w.lower() in stop_words]\n",
    "    s = \" \".join(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf4e275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered topics with threshold 0.5 have been saved\n",
      "\n",
      "Filtered topics with threshold 0.6 have been saved\n",
      "\n",
      "Filtered topics with threshold 0.7 have been saved\n",
      "\n",
      "Filtered topics with threshold 0.8 have been saved\n",
      "\n",
      "Filtered topics with threshold 0.9 have been saved\n"
     ]
    }
   ],
   "source": [
    "# Read topics from the CSV file and append them to the topic list\n",
    "with open(\"data\\\\supervisors_list.csv\", mode=\"r\", encoding=\"utf-8-sig\") as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        # Parse the list of topics if present\n",
    "        for field in [\"Expertise Area 1\", \"Expertise Area 2\", \"Expertise Area 3\"]:\n",
    "            if row.get(field, \"\"):\n",
    "                try:\n",
    "                    # Convert the string representation of a list into an actual list\n",
    "                    topic_list = eval(row.get(field, \"[]\"))\n",
    "                    if isinstance(topic_list, list):\n",
    "                        # Exclude \"N/A\" and add valid topics to the list\n",
    "                        initial_topics.extend([topic for topic in topic_list if topic and topic != \"N/A\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing topics in field '{field}': {e}\")\n",
    "\n",
    "new_topics = []\n",
    "\n",
    "# Read topics from the staff_proflies CSV file and append them to a new list\n",
    "with open(\"data\\\\staff_profiles.csv\", mode=\"r\", encoding=\"utf-8-sig\") as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        # Parse the list of topics if present\n",
    "        for field in [\"research_interests\", \"teaching_areas\", \"courses_taught\"]:\n",
    "            if row.get(field, \"\"):\n",
    "                try:\n",
    "                    # Convert the string representation of a list into an actual list\n",
    "                    topic_list = eval(row.get(field, \"[]\"))\n",
    "                    if isinstance(topic_list, list):\n",
    "                        # Exclude \"N/A\" and add valid topics to the list\n",
    "                        new_topics.extend([topic for topic in topic_list if topic and topic != \"N/A\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing topics in field '{field}': {e}\")\n",
    "\n",
    "topics = list(set(initial_topics))\n",
    "new_topics = list(set(new_topics))\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute embeddings for all topics\n",
    "topic_embeddings = model.encode(topics, convert_to_tensor=True)\n",
    "new_topics_embeddings = model.encode(new_topics, convert_to_tensor=True)\n",
    "\n",
    "# Identify and remove similar topics\n",
    "def find_similar_topics(embeddings, selected_topics, threshold=0.7):\n",
    "    unique_topics = []\n",
    "    for i, topic in enumerate(selected_topics):\n",
    "        is_similar = False\n",
    "        for j, unique_topic in enumerate(unique_topics):\n",
    "            sim = util.cos_sim(embeddings[i], embeddings[j])\n",
    "            if sim > threshold:\n",
    "                is_similar = True\n",
    "                break\n",
    "        if not is_similar:\n",
    "            unique_topics.append(topic)\n",
    "    return unique_topics\n",
    "\n",
    "# Create a loop where the threshold increments by 0.1 for each iteration\n",
    "# In each loop, use find_similar_topics and save the results in a csv file with the threshold in the filename\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for threshold in thresholds:\n",
    "    filtered_topics = find_similar_topics(topic_embeddings, topics, threshold)\n",
    "    \n",
    "    # Save the filtered topics to a CSV file\n",
    "    filename = f\"data\\\\filtered_topics_threshold_{threshold}.csv\"\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Topic\"])  # Write the header\n",
    "        for topic in filtered_topics:\n",
    "            writer.writerow([clean_text(topic, comma=True)])  # Write each topic as a row\n",
    "\n",
    "    filtered_new_topics = find_similar_topics(new_topics_embeddings, new_topics, threshold)\n",
    "    filtered_combined_topics = list(set(filtered_topics + filtered_new_topics))\n",
    "    filtered_combined_topics = find_similar_topics(model.encode(filtered_combined_topics, convert_to_tensor=True), filtered_combined_topics, threshold)\n",
    "\n",
    "    # Save the combined filtered topics to a CSV file\n",
    "    filename_combined = f\"data\\\\filtered_combined_topics_threshold_{threshold}.csv\"\n",
    "    with open(filename_combined, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Topic\"])\n",
    "        for topic in filtered_combined_topics:\n",
    "            writer.writerow([clean_text(topic, comma=True)])\n",
    "\n",
    "    print(f\"\\nFiltered topics with threshold {threshold} have been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c2532",
   "metadata": {},
   "source": [
    "Create fucntions to label students based on the list of topcis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0815f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from transformers import pipeline\n",
    "from keybert import KeyBERT\n",
    "\n",
    "\n",
    "# Load sentiment analysis model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", device=0)  # Use GPU if available\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "kw_model = KeyBERT(model)\n",
    "\n",
    "def extract_candidate_topics(sentence, top_n=1):\n",
    "    # Use KeyBERT to extract top N keywords/phrases from the sentence\n",
    "    keywords = kw_model.extract_keywords(sentence, top_n=top_n, stop_words='english')\n",
    "    # Return keywords as a list of strings\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "def predict_match(student_text, lecturer_topics, threshold=0.5):\n",
    "    student_sentences = student_text.split(\". \")  # Split student text into sentences\n",
    "    positive_topics = []\n",
    "    negative_topics = []\n",
    "\n",
    "    for sentence in student_sentences:\n",
    "        clean_sentence = clean_text(sentence)\n",
    "        student_emb = model.encode(clean_sentence, convert_to_tensor=True)\n",
    "        topic_embs = model.encode(lecturer_topics, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute similarities between the sentence and lecturer topics\n",
    "        similarities = util.cos_sim(student_emb, topic_embs)\n",
    "        max_sim = float(similarities.max())  # Get the maximum similarity score\n",
    "\n",
    "        # Check if there is any similarity in existing topics\n",
    "        # If not, extract a new topic\n",
    "        # and classify it as positive or negative\n",
    "        if max_sim < threshold:\n",
    "            new_topics = extract_candidate_topics(clean_sentence)\n",
    "            for kw in new_topics:\n",
    "                kw_emb = model.encode(kw, convert_to_tensor=True)\n",
    "                similarities = util.cos_sim(kw_emb, topic_embs)\n",
    "                max_sim_kw = float(similarities.max())  # Get the maximum similarity score\n",
    "                if max_sim_kw < threshold:\n",
    "                    lecturer_topics.append(kw)\n",
    "                    # print(f\"New topic added: {kw}\")\n",
    "                    # print(f\"Sentence: {sentence}\")\n",
    "                else:\n",
    "                    topic = lecturer_topics[similarities.argmax()]\n",
    "                    sentiment = sentiment_analyzer(clean_sentence)[0]\n",
    "                    if sentiment[\"label\"] == \"NEGATIVE\":\n",
    "                        negative_topics.append(clean_text(topic))\n",
    "                    else:\n",
    "                        positive_topics.append(clean_text(topic))\n",
    "            # sentiment = sentiment_analyzer(clean_sentence)[0]\n",
    "            # if sentiment[\"label\"] == \"NEGATIVE\":\n",
    "            #     negative_topics.append(new_topic)\n",
    "            # else:\n",
    "            #     positive_topics.append(new_topic)\n",
    "            # continue\n",
    "\n",
    "        # Match all relevant topics in the sentence\n",
    "        for idx, sim in enumerate(similarities[0]):\n",
    "            if sim > threshold:\n",
    "                topic = lecturer_topics[idx]\n",
    "\n",
    "                # Use sentiment analysis to classify the sentence\n",
    "                sentiment = sentiment_analyzer(clean_sentence)[0]\n",
    "                if sentiment[\"label\"] == \"NEGATIVE\":\n",
    "                    negative_topics.append(clean_text(topic))\n",
    "                else:\n",
    "                    positive_topics.append(clean_text(topic))\n",
    "\n",
    "    return {\n",
    "        \"positive_topics\": list(set(positive_topics)),  # Remove duplicates\n",
    "        \"negative_topics\": list(set(negative_topics))   # Remove duplicates\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c866ff0",
   "metadata": {},
   "source": [
    "Utilise LLM generated sentences to test and determine accuracy of model, hyper-tuning begins here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b056b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.5):\n",
      "Accuracy: 0.445\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.43      0.35      0.39       200\n",
      "Positive Interest       0.45      0.54      0.49       200\n",
      "\n",
      "         accuracy                           0.45       400\n",
      "        macro avg       0.44      0.45      0.44       400\n",
      "     weighted avg       0.44      0.45      0.44       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.5):\n",
      "Accuracy: 0.033\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.09      0.05      0.06       525\n",
      "\n",
      "         accuracy                           0.03       789\n",
      "        macro avg       0.04      0.02      0.03       789\n",
      "     weighted avg       0.06      0.03      0.04       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.5):\n",
      "Accuracy: 0.031\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.03      0.03      0.03       145\n",
      "Positive Interest       0.03      0.03      0.03       145\n",
      "\n",
      "         accuracy                           0.03       290\n",
      "        macro avg       0.03      0.03      0.03       290\n",
      "     weighted avg       0.03      0.03      0.03       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.5):\n",
      "Accuracy: 0.133\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.08      0.13      0.10        71\n",
      "Positive Interest       0.22      0.14      0.17       124\n",
      "\n",
      "         accuracy                           0.13       195\n",
      "        macro avg       0.15      0.13      0.13       195\n",
      "     weighted avg       0.17      0.13      0.14       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.5):\n",
      "Accuracy: 0.034\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.03      0.07       118\n",
      "\n",
      "         accuracy                           0.03       118\n",
      "        macro avg       0.50      0.02      0.03       118\n",
      "     weighted avg       1.00      0.03      0.07       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.5):\n",
      "Accuracy: 0.625\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.61      0.71      0.66       200\n",
      "Positive Interest       0.65      0.54      0.59       200\n",
      "\n",
      "         accuracy                           0.62       400\n",
      "        macro avg       0.63      0.62      0.62       400\n",
      "     weighted avg       0.63      0.62      0.62       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.5):\n",
      "Accuracy: 0.052\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.13      0.08      0.10       525\n",
      "\n",
      "         accuracy                           0.05       789\n",
      "        macro avg       0.07      0.04      0.05       789\n",
      "     weighted avg       0.09      0.05      0.07       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.5):\n",
      "Accuracy: 0.059\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.06      0.06      0.06       145\n",
      "Positive Interest       0.06      0.06      0.06       145\n",
      "\n",
      "         accuracy                           0.06       290\n",
      "        macro avg       0.06      0.06      0.06       290\n",
      "     weighted avg       0.06      0.06      0.06       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.5):\n",
      "Accuracy: 0.169\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.12      0.20      0.15        71\n",
      "Positive Interest       0.25      0.15      0.19       124\n",
      "\n",
      "         accuracy                           0.17       195\n",
      "        macro avg       0.18      0.18      0.17       195\n",
      "     weighted avg       0.20      0.17      0.17       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.5):\n",
      "Accuracy: 0.059\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.06      0.11       118\n",
      "\n",
      "         accuracy                           0.06       118\n",
      "        macro avg       0.50      0.03      0.06       118\n",
      "     weighted avg       1.00      0.06      0.11       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.5):\n",
      "Accuracy: 0.490\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.49      0.71      0.58       200\n",
      "Positive Interest       0.48      0.27      0.34       200\n",
      "\n",
      "         accuracy                           0.49       400\n",
      "        macro avg       0.49      0.49      0.46       400\n",
      "     weighted avg       0.49      0.49      0.46       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.5):\n",
      "Accuracy: 0.054\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.14      0.08      0.10       525\n",
      "\n",
      "         accuracy                           0.05       789\n",
      "        macro avg       0.07      0.04      0.05       789\n",
      "     weighted avg       0.09      0.05      0.07       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.5):\n",
      "Accuracy: 0.079\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.07      0.07      0.07       145\n",
      "Positive Interest       0.09      0.09      0.09       145\n",
      "\n",
      "         accuracy                           0.08       290\n",
      "        macro avg       0.08      0.08      0.08       290\n",
      "     weighted avg       0.08      0.08      0.08       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.5):\n",
      "Accuracy: 0.205\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.14      0.23      0.17        71\n",
      "Positive Interest       0.30      0.19      0.24       124\n",
      "\n",
      "         accuracy                           0.21       195\n",
      "        macro avg       0.22      0.21      0.20       195\n",
      "     weighted avg       0.24      0.21      0.21       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.5):\n",
      "Accuracy: 0.110\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.11      0.20       118\n",
      "\n",
      "         accuracy                           0.11       118\n",
      "        macro avg       0.50      0.06      0.10       118\n",
      "     weighted avg       1.00      0.11      0.20       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.8.csv at threshold (0.5):\n",
      "Accuracy: 0.672\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.66      0.71      0.69       200\n",
      "Positive Interest       0.69      0.63      0.66       200\n",
      "\n",
      "         accuracy                           0.67       400\n",
      "        macro avg       0.67      0.67      0.67       400\n",
      "     weighted avg       0.67      0.67      0.67       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.8.csv at threshold (0.5):\n",
      "Accuracy: 0.065\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.16      0.10      0.12       525\n",
      "\n",
      "         accuracy                           0.06       789\n",
      "        macro avg       0.08      0.05      0.06       789\n",
      "     weighted avg       0.11      0.06      0.08       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.8.csv at threshold (0.5):\n",
      "Accuracy: 0.100\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.10      0.10      0.10       145\n",
      "Positive Interest       0.10      0.10      0.10       145\n",
      "\n",
      "         accuracy                           0.10       290\n",
      "        macro avg       0.10      0.10      0.10       290\n",
      "     weighted avg       0.10      0.10      0.10       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.8.csv at threshold (0.5):\n",
      "Accuracy: 0.328\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.21      0.30      0.24        71\n",
      "Positive Interest       0.46      0.35      0.40       124\n",
      "\n",
      "         accuracy                           0.33       195\n",
      "        macro avg       0.33      0.32      0.32       195\n",
      "     weighted avg       0.37      0.33      0.34       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.8.csv at threshold (0.5):\n",
      "Accuracy: 0.144\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.14      0.25       118\n",
      "\n",
      "         accuracy                           0.14       118\n",
      "        macro avg       0.50      0.07      0.13       118\n",
      "     weighted avg       1.00      0.14      0.25       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.9.csv at threshold (0.5):\n",
      "Accuracy: 0.833\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.93      0.71      0.81       200\n",
      "Positive Interest       0.77      0.95      0.85       200\n",
      "\n",
      "         accuracy                           0.83       400\n",
      "        macro avg       0.85      0.83      0.83       400\n",
      "     weighted avg       0.85      0.83      0.83       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.9.csv at threshold (0.5):\n",
      "Accuracy: 0.075\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.18      0.11      0.14       525\n",
      "\n",
      "         accuracy                           0.07       789\n",
      "        macro avg       0.09      0.06      0.07       789\n",
      "     weighted avg       0.12      0.07      0.09       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.9.csv at threshold (0.5):\n",
      "Accuracy: 0.117\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.12      0.12      0.12       145\n",
      "Positive Interest       0.12      0.12      0.12       145\n",
      "\n",
      "         accuracy                           0.12       290\n",
      "        macro avg       0.12      0.12      0.12       290\n",
      "     weighted avg       0.12      0.12      0.12       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.9.csv at threshold (0.5):\n",
      "Accuracy: 0.344\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.21      0.30      0.25        71\n",
      "Positive Interest       0.48      0.37      0.42       124\n",
      "\n",
      "         accuracy                           0.34       195\n",
      "        macro avg       0.35      0.33      0.33       195\n",
      "     weighted avg       0.38      0.34      0.36       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.9.csv at threshold (0.5):\n",
      "Accuracy: 0.161\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.16      0.28       118\n",
      "\n",
      "         accuracy                           0.16       118\n",
      "        macro avg       0.50      0.08      0.14       118\n",
      "     weighted avg       1.00      0.16      0.28       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.6):\n",
      "Accuracy: 0.203\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.19      0.19      0.19       200\n",
      "Positive Interest       0.21      0.21      0.21       200\n",
      "\n",
      "         accuracy                           0.20       400\n",
      "        macro avg       0.20      0.20      0.20       400\n",
      "     weighted avg       0.20      0.20      0.20       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.6):\n",
      "Accuracy: 0.020\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.06      0.03      0.04       525\n",
      "\n",
      "         accuracy                           0.02       789\n",
      "        macro avg       0.03      0.02      0.02       789\n",
      "     weighted avg       0.04      0.02      0.03       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.6):\n",
      "Accuracy: 0.021\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.01      0.01      0.01       145\n",
      "Positive Interest       0.03      0.03      0.03       145\n",
      "\n",
      "         accuracy                           0.02       290\n",
      "        macro avg       0.02      0.02      0.02       290\n",
      "     weighted avg       0.02      0.02      0.02       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.6):\n",
      "Accuracy: 0.108\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.07      0.11      0.08        71\n",
      "Positive Interest       0.17      0.10      0.13       124\n",
      "\n",
      "         accuracy                           0.11       195\n",
      "        macro avg       0.12      0.11      0.11       195\n",
      "     weighted avg       0.13      0.11      0.11       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.5.csv at threshold (0.6):\n",
      "Accuracy: 0.025\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.03      0.05       118\n",
      "\n",
      "         accuracy                           0.03       118\n",
      "        macro avg       0.50      0.01      0.02       118\n",
      "     weighted avg       1.00      0.03      0.05       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.6):\n",
      "Accuracy: 0.403\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.43      0.59      0.50       200\n",
      "Positive Interest       0.34      0.21      0.26       200\n",
      "\n",
      "         accuracy                           0.40       400\n",
      "        macro avg       0.39      0.40      0.38       400\n",
      "     weighted avg       0.39      0.40      0.38       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.6):\n",
      "Accuracy: 0.029\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.08      0.04      0.06       525\n",
      "\n",
      "         accuracy                           0.03       789\n",
      "        macro avg       0.04      0.02      0.03       789\n",
      "     weighted avg       0.05      0.03      0.04       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.6):\n",
      "Accuracy: 0.041\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.03      0.03      0.03       145\n",
      "Positive Interest       0.05      0.05      0.05       145\n",
      "\n",
      "         accuracy                           0.04       290\n",
      "        macro avg       0.04      0.04      0.04       290\n",
      "     weighted avg       0.04      0.04      0.04       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.6):\n",
      "Accuracy: 0.144\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.11      0.20      0.14        71\n",
      "Positive Interest       0.20      0.11      0.14       124\n",
      "\n",
      "         accuracy                           0.14       195\n",
      "        macro avg       0.16      0.16      0.14       195\n",
      "     weighted avg       0.17      0.14      0.14       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.6.csv at threshold (0.6):\n",
      "Accuracy: 0.034\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.03      0.07       118\n",
      "\n",
      "         accuracy                           0.03       118\n",
      "        macro avg       0.50      0.02      0.03       118\n",
      "     weighted avg       1.00      0.03      0.07       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.6):\n",
      "Accuracy: 0.388\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.42      0.59      0.49       200\n",
      "Positive Interest       0.31      0.18      0.23       200\n",
      "\n",
      "         accuracy                           0.39       400\n",
      "        macro avg       0.37      0.39      0.36       400\n",
      "     weighted avg       0.37      0.39      0.36       400\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/claude_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.6):\n",
      "Accuracy: 0.039\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00       264\n",
      "Positive Interest       0.11      0.06      0.08       525\n",
      "\n",
      "         accuracy                           0.04       789\n",
      "        macro avg       0.05      0.03      0.04       789\n",
      "     weighted avg       0.07      0.04      0.05       789\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/deepseek_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.6):\n",
      "Accuracy: 0.059\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.05      0.05      0.05       145\n",
      "Positive Interest       0.07      0.07      0.07       145\n",
      "\n",
      "         accuracy                           0.06       290\n",
      "        macro avg       0.06      0.06      0.06       290\n",
      "     weighted avg       0.06      0.06      0.06       290\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/gemini_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.6):\n",
      "Accuracy: 0.179\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.13      0.21      0.16        71\n",
      "Positive Interest       0.26      0.16      0.20       124\n",
      "\n",
      "         accuracy                           0.18       195\n",
      "        macro avg       0.19      0.19      0.18       195\n",
      "     weighted avg       0.21      0.18      0.18       195\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\seanh\\Documents\\University\\CAPSTONE 2\\CAPSTONE-2\\.env_capstone\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data/linkedin_sentences.csv and data/filtered_combined_topics_threshold_0.7.csv at threshold (0.6):\n",
      "Accuracy: 0.076\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.00      0.00      0.00         0\n",
      "Positive Interest       1.00      0.08      0.14       118\n",
      "\n",
      "         accuracy                           0.08       118\n",
      "        macro avg       0.50      0.04      0.07       118\n",
      "     weighted avg       1.00      0.08      0.14       118\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for data/cgpt_sentences.csv and data/filtered_combined_topics_threshold_0.8.csv at threshold (0.6):\n",
      "Accuracy: 0.530\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Negative Interest       0.53      0.59      0.56       200\n",
      "Positive Interest       0.53      0.47      0.50       200\n",
      "\n",
      "         accuracy                           0.53       400\n",
      "        macro avg       0.53      0.53      0.53       400\n",
      "     weighted avg       0.53      0.53      0.53       400\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# List of CSV files to process\n",
    "sentence_files = [\n",
    "    \"data/cgpt_sentences.csv\",\n",
    "    \"data/claude_sentences.csv\",\n",
    "    \"data/deepseek_sentences.csv\",\n",
    "    \"data/gemini_sentences.csv\",\n",
    "    \"data/linkedin_sentences.csv\"\n",
    "]\n",
    "topic_files = [\n",
    "    \"data/filtered_combined_topics_threshold_0.5.csv\",\n",
    "    \"data/filtered_combined_topics_threshold_0.6.csv\", \n",
    "    \"data/filtered_combined_topics_threshold_0.7.csv\",\n",
    "    \"data/filtered_combined_topics_threshold_0.8.csv\",\n",
    "    \"data/filtered_combined_topics_threshold_0.9.csv\",\n",
    "]\n",
    "\n",
    "# Threshold for topic matching\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Lecturer topics\n",
    "lecturer_topics = open(\"data/filtered_topics.csv\", mode=\"r\", encoding=\"utf-8\").read().splitlines()[1:]  # Skip header\n",
    "\n",
    "# Function to evaluate accuracy and generate classification report\n",
    "def evaluate_predict_match(file_path, lecturer_topics, threshold):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the CSV has the required columns\n",
    "    if \"sentence\" not in df.columns or \"positive_topic\" not in df.columns or \"negative_topic\" not in df.columns:\n",
    "        print(f\"Error: {file_path} does not contain the required columns.\")\n",
    "        return\n",
    "\n",
    "    # Extract true labels for positive and negative topics\n",
    "    true_positive_topics = df[\"positive_topic\"].apply(lambda x: x.split(\",\") if pd.notna(x) else [])\n",
    "    true_negative_topics = df[\"negative_topic\"].apply(lambda x: x.split(\",\") if pd.notna(x) else [])\n",
    "\n",
    "    # Predicted labels\n",
    "    predicted_positive_topics = []\n",
    "    predicted_negative_topics = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        student_text = row[\"sentence\"]\n",
    "\n",
    "        # Use the predict_match function to get positive and negative topics\n",
    "        result = predict_match(student_text, open(lecturer_topics, mode=\"r\", encoding=\"utf-8\").read().splitlines(), threshold)\n",
    "\n",
    "        # Append predicted topics\n",
    "        predicted_positive_topics.append(result[\"positive_topics\"])\n",
    "        predicted_negative_topics.append(result[\"negative_topics\"])\n",
    "\n",
    "    # Flatten true and predicted labels for evaluation\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # Positive topics\n",
    "        for topic in true_positive_topics[i]:\n",
    "            true_labels.append(1)\n",
    "            found = False\n",
    "            for pred_topic in predicted_positive_topics[i]:\n",
    "                pred_topic_emb = model.encode(pred_topic, convert_to_tensor=True)\n",
    "                true_topic_emb = model.encode(topic, convert_to_tensor=True)\n",
    "                similarity = util.cos_sim(pred_topic_emb, true_topic_emb)\n",
    "                if similarity > 0.8:\n",
    "                    predicted_labels.append(1)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                predicted_labels.append(0)\n",
    "\n",
    "        # Negative topics\n",
    "        for topic in true_negative_topics[i]:\n",
    "            true_labels.append(0)\n",
    "            found = False\n",
    "            for pred_topic in predicted_negative_topics[i]:\n",
    "                pred_topic_emb = model.encode(pred_topic, convert_to_tensor=True)\n",
    "                true_topic_emb = model.encode(topic, convert_to_tensor=True)\n",
    "                similarity = util.cos_sim(pred_topic_emb, true_topic_emb)\n",
    "                if similarity > 0.8:\n",
    "                    predicted_labels.append(0)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                predicted_labels.append(1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=[\"Negative Interest\", \"Positive Interest\"])\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Results for {file_path} and {lecturer_topics} at threshold ({threshold}):\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # store report in a text file\n",
    "    with open(f\"results/report_{file_path.split('/')[-1].split('.')[0]}_{lecturer_topics}_threshold_{threshold}.txt\", \"w\") as report_file:\n",
    "        report_file.write(f\"Results for {file_path} and {lecturer_topics} at threshold ({threshold}):\\n\")\n",
    "        report_file.write(f\"Accuracy: {accuracy:.3f}\\n\")\n",
    "        report_file.write(\"Classification Report:\\n\")\n",
    "        report_file.write(report)\n",
    "        report_file.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    # Store results in a CSV file\n",
    "    results_df = pd.DataFrame({\n",
    "        \"sentence\": df[\"sentence\"],\n",
    "        \"predicted_positive_topics\": predicted_positive_topics,\n",
    "        \"predicted_negative_topics\": predicted_negative_topics,\n",
    "        \"true_positive_topics\": true_positive_topics,\n",
    "        \"true_negative_topics\": true_negative_topics\n",
    "    })\n",
    "    results_df.to_csv(f\"results/results_{file_path.split('/')[-1].split('.')[0]}_{lecturer_topics}_threshold_{threshold}.csv\", index=False)\n",
    "\n",
    "    # Print the predicted topics for each sentence\n",
    "    # Print the true topics for each sentence\n",
    "    # for i, row in df.iterrows():\n",
    "    #     student_text = row[\"sentence\"]\n",
    "    #     print(f\"Sentence: {student_text}\")\n",
    "    #     print(f\"Predicted Positive Topics: {predicted_positive_topics[i]}\")\n",
    "    #     print(f\"Predicted Negative Topics: {predicted_negative_topics[i]}\")\n",
    "    #     print(f\"True Positive Topics: {true_positive_topics[i]}\")\n",
    "    #     print(f\"True Negative Topics: {true_negative_topics[i]}\")\n",
    "    #     print(\"-\" * 50)\n",
    "\n",
    "# Process each CSV file\n",
    "for threshold in thresholds:\n",
    "    for lecturer_topics in topic_files:\n",
    "        for sentence_file in sentence_files:\n",
    "            evaluate_predict_match(sentence_file, lecturer_topics,threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
